{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fake_News = pd.read_csv(\"https://raw.githubusercontent.com/ba18406/Lab001/master/fake_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical variables in dataSet\n",
    "encoder1 = LabelEncoder()\n",
    "Fake_News['Usage'] = encoder1.fit_transform(Fake_News['Usage'])\n",
    "\n",
    "encoder2 = LabelEncoder()\n",
    "Fake_News['Expected'] = encoder2.fit_transform(Fake_News['Expected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataSet 3\n",
      "Imbalance:  85.73995379745031 %\n",
      "Counter({0: 120252, 1: 20000})\n"
     ]
    }
   ],
   "source": [
    "Usage_data = Counter(Fake_News['Usage'])\n",
    "print(\"\\nDataSet 3\\nImbalance: \", (Usage_data[0]/(Usage_data[0]+Usage_data[1]))*100, \"%\")\n",
    "print(Usage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dependant and independant variables\n",
    "X = Fake_News.iloc[:, 0:1]\n",
    "Y = Fake_News.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 74.27980808750137), (4, 74.28979005884922), (5, 74.28479917484539), (6, 74.28194712493452), (7, 74.29406843872583), (8, 74.30333810928673), (9, 74.32829364767697), (10, 74.4152810682885), (11, 74.50939957954327), (12, 74.77677895367731), (13, 74.93292695790579), (14, 74.94790103329873), (15, 74.99923681332326), (16, 74.96643778183272), (17, 75.03845295711326), (18, 75.11973424450085), (19, 75.13114478255687)]\n"
     ]
    }
   ],
   "source": [
    "#Finding the best depth for tree by performing a grid search.\n",
    "#Additionaly performing 10 Fold cross validation of each classifier \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "depth = []\n",
    "for i in range(3,20):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i)\n",
    "    # Perform 10-fold cross validation \n",
    "    scores = cross_val_score(estimator=clf, X=X, y=Y, cv=10, n_jobs=4)\n",
    "    depth.append((i,scores.mean()*100))\n",
    "print(depth)\n",
    "#Hence we can achieve around 85% accuracy using a decision tree if we set max depth of the tree from 5-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3259703  0.16248596 0.1651067  0.59628104 0.78971671 0.79208786\n",
      " 0.7959311  0.79306041 0.7978033  0.79518223]\n",
      "('Mean Score: ', 0.6013625605520336)\n"
     ]
    }
   ],
   "source": [
    "#Training a Random Forest Classifier.\n",
    "#Also performing 10 Fold Cross Validation.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomforest = RandomForestClassifier(n_estimators=100)\n",
    "randomforest.fit(X, Y)\n",
    "RandomForest_CV_Score = cross_val_score(randomforest, X, Y, cv=10)\n",
    "print(RandomForest_CV_Score)\n",
    "print(\"Mean Score: \", RandomForest_CV_Score.mean())\n",
    "#Hence around 85% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data into 10 equal portions using stratified K Fold. \n",
    "#Using 1 portion for testing and 9 for training.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "bins = StratifiedKFold(n_splits = 10)\n",
    "for train_index, test_index in bins.split(X,Y):\n",
    "    #print(test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    training_data, testing_data = Fake_News.iloc[train_index], Fake_News.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now finding the ideal number of clusters.\n",
    "#Using Elbow Method\n",
    "\n",
    "#mms = MinMaxScaler()\n",
    "#data_transformed= mms.fit_transform(X_train)\n",
    "\n",
    "inertiaS = []\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters = k)\n",
    "    #print(km.inertia_)\n",
    "    cluster_labels = km.fit_predict(X_train)\n",
    "    inertiaS.append(km.inertia_)\n",
    "    avg_score = silhouette_score(X_train, cluster_labels)\n",
    "    print(\"For n_clusters =\", k, \"The average silhouette_score is :\", avg_score)\n",
    "    \n",
    "plt.plot(range(2, 11), inertiaS)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "#Hence according to Elbow method and silhoutte analysis, the ideal number of clusters should be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running K-means on the selected number of clusters, i.e: 2\n",
    "km = KMeans(n_clusters = 2, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "km.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels = km.fit_predict(X_train)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id cluster =  0 labels = 1\n"
     ]
    }
   ],
   "source": [
    "for a,b,c in zip(X_train, cluster_labels , Y_train):\n",
    "    print(a, \"cluster = \", b, \"labels =\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num minority class samples in cluster(0): 1\n",
      "total num minority class samples in cluster(1): 0\n"
     ]
    }
   ],
   "source": [
    "#storing cluster number and respective centroids with number of samples\n",
    "label0 = np.array([]) # cluster 0 with minority class\n",
    "label1 = np.array([])# cluster 1 with minority class\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "numOfLabels0 = 0\n",
    "numOfLabels1 = 0\n",
    "for cluster,centroid,labels in zip(km.labels_,X_train,Y_train):\n",
    "    if(cluster==0 and labels == 1):\n",
    "        count0 = count0 + 1\n",
    "        label0 = np.append(label0 ,[centroid,\"cluster\",cluster,'labels',labels])\n",
    "        numOfLabels0 = count0\n",
    "    elif(cluster==1 and labels == 1):\n",
    "        count1 = count1 + 1\n",
    "        label1 = np.append(label1 ,[centroid,\"cluster\",cluster,'labels',labels])\n",
    "        numOfLabels1 = count1\n",
    "        \n",
    "print('total num minority class samples in cluster(0):',numOfLabels0)\n",
    "print('total num minority class samples in cluster(1):',numOfLabels1)\n",
    "\n",
    "#centroid of cluster-1 \n",
    "#print(\"Label 0:\",label0)\n",
    "#centroid of cluster-2\n",
    "#print(\"label 1:\",label1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
